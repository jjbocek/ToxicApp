{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sandbox to test code, post ideas, etc. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#from sklearn's...\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Convert a collection of text documents to a matrix of token counts.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #Transform a count matrix to a normalized tf or tf-idf representation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We have three datasets we are using; training (**train_ds**), testing (**test_ds**), and testing labels (**test_labels_ds**)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training and test dataframes from file; preprocessing completed in 'text_preprocessing' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('clean_data1.p', 'rb')\n",
    "clean_data = pickle.load(file_object)\n",
    "train_df = clean_data[0]\n",
    "test_df = clean_data[1]\n",
    "test_lab = clean_data[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>non_toxic</th>\n",
       "      <th>overall_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edit make username hardcore metall...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww match background colour im seemingly stic...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really try edit war guy constantly ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  non_toxic  overall_toxic\n",
       "0  explanation edit make username hardcore metall...          1              0\n",
       "1  daww match background colour im seemingly stic...          1              0\n",
       "2  hey man im really try edit war guy constantly ...          1              0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity_level\n",
      "4                 1595\n",
      "1                 1433\n",
      "3                 1370\n",
      "2                  931\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(train_ds[['toxicity_level']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule succesful youll ever whats ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>rfc title fine imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>source zawe ashton lapland —</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  yo bitch ja rule succesful youll ever whats ha...\n",
       "1  0000247867823ef7                                 rfc title fine imo\n",
       "2  00013b17ad220c46                       source zawe ashton lapland —"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non_toxic</th>\n",
       "      <th>overall_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   non_toxic  overall_toxic\n",
       "0          2              1\n",
       "1          2              1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lab.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset1: Create a subsample of data for quickened runtime with 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset of testing data; removed the id column\n",
    "train_df_subset = train_df.loc[1:1000,'comment_text']\n",
    "train_lab_subset = train_df.loc[1:1000,'overall_toxic']\n",
    "\n",
    "#subset of testing data; removed the id column\n",
    "test_df_subset = test_df.loc[1:1000,'comment_text']\n",
    "test_lab_subset = test_lab.loc[1:1000,'overall_toxic']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978\n",
      "63978\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset3: Create a subsample of data for quickened run time with toxcity_level as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#subset of testing data; removed the id column\n",
    "subset3 = train_df.sample(n = 1000, random_state= 123)\n",
    "train_df_subset3 = subset3['comment_text']\n",
    "train_lab_subset3 = subset3['toxicity_level']\n",
    "\n",
    "#subset of testing data; removed the id column\n",
    "subset3_test = test_df.sample(n = 1000, random_state= 123)\n",
    "subset3_test_lab = test_lab.sample(n = 1000, random_state= 123)\n",
    "train_df_subset3 = subset3_test['comment_text']\n",
    "train_lab_subset3 = subset3_test_lab['toxicity_level']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df_subset3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 1, 3, 2, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df.loc[:,'toxic':'identity_hate'].sum(axis =1).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Could convert the comment labels to one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>gay antisemmitian archangel white tiger meow g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>00472b8e2d38d1ea</td>\n",
       "      <td>pair jewhating weiner nazi schmuck</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>006b94add72ed61c</td>\n",
       "      <td>think fagget get oife burn hell hate sorry can...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>008e0818dde894fb</td>\n",
       "      <td>kill nigger hard others say include racist som...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0097dd5c29bf7a15</td>\n",
       "      <td>u r tw fuck u gay boyu r smellyfuck ur mum poopie</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159281</th>\n",
       "      <td>fb726deec64157bd</td>\n",
       "      <td>lol youre gay never know good feel fuck woman as</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159336</th>\n",
       "      <td>fc3efa2f6f025f6d</td>\n",
       "      <td>oh fuck pansy jew would whine bnai brith beat ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159400</th>\n",
       "      <td>fd052883fa6a8697</td>\n",
       "      <td>shalom semite get fuck kill son bitch dont lea...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159449</th>\n",
       "      <td>fdce660ddcd6d7ca</td>\n",
       "      <td>think gay fag</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159494</th>\n",
       "      <td>fef4cf7ba0012866</td>\n",
       "      <td>previous conversation fuck shit eat liberal ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1302 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "42      001810bf8c45bf5f  gay antisemmitian archangel white tiger meow g...   \n",
       "105     00472b8e2d38d1ea                 pair jewhating weiner nazi schmuck   \n",
       "176     006b94add72ed61c  think fagget get oife burn hell hate sorry can...   \n",
       "218     008e0818dde894fb  kill nigger hard others say include racist som...   \n",
       "238     0097dd5c29bf7a15  u r tw fuck u gay boyu r smellyfuck ur mum poopie   \n",
       "...                  ...                                                ...   \n",
       "159281  fb726deec64157bd   lol youre gay never know good feel fuck woman as   \n",
       "159336  fc3efa2f6f025f6d  oh fuck pansy jew would whine bnai brith beat ...   \n",
       "159400  fd052883fa6a8697  shalom semite get fuck kill son bitch dont lea...   \n",
       "159449  fdce660ddcd6d7ca                                      think gay fag   \n",
       "159494  fef4cf7ba0012866  previous conversation fuck shit eat liberal ma...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "42          1             0        1       0       1              1  \n",
       "105         1             0        1       0       1              1  \n",
       "176         1             0        1       1       1              1  \n",
       "218         1             0        1       0       1              1  \n",
       "238         1             0        1       0       1              1  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159281      1             1        1       0       1              1  \n",
       "159336      1             0        1       0       1              1  \n",
       "159400      1             1        1       1       1              1  \n",
       "159449      1             0        0       0       0              1  \n",
       "159494      1             0        1       0       1              1  \n",
       "\n",
       "[1302 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df[(train_df['toxic'] == 1) & (train_df['identity_hate'] == 1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample how we did it in homework just on 5 comments as corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get term count matrix along with terms array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['accident', 'actual', 'appear', 'article', 'background', 'backlog',\n",
       "       'cant', 'care', 'chance', 'closure', 'colour', 'constantly',\n",
       "       'date', 'daww', 'delay', 'doll', 'dont', 'edit', 'eg', 'else',\n",
       "       'etc', 'exact', 'explanation', 'fac', 'fan', 'first', 'form',\n",
       "       'format', 'gas', 'guess', 'guy', 'hardcore', 'hero', 'hey', 'ie',\n",
       "       'im', 'improvement', 'info', 'information', 'instead', 'january',\n",
       "       'know', 'later', 'let', 'list', 'make', 'man', 'match', 'may',\n",
       "       'metallica', 'need', 'new', 'noone', 'page', 'please',\n",
       "       'preference', 'real', 'really', 'reference', 'relevant',\n",
       "       'remember', 'remove', 'retire', 'revert', 'review', 'reviewer',\n",
       "       'section', 'seem', 'seemingly', 'since', 'sir', 'statistic',\n",
       "       'stick', 'style', 'subsection', 'suggestion', 'talk', 'template',\n",
       "       'thank', 'thats', 'think', 'tidy', 'try', 'turn', 'type',\n",
       "       'username', 'utc', 'vandalism', 'vote', 'want', 'war', 'werent',\n",
       "       'wikipediagoodarticlenominationstransport', 'wonder', 'york'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = train_df['comment_text']\n",
    "c1 = corpus.head(5)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(c1)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  85  86  87  88  89  90  91  \\\n",
       "0   0   0   0   0   0   0   0   0   0   1  ...   1   0   1   1   0   0   1   \n",
       "1   0   0   0   0   1   0   0   0   0   0  ...   0   1   0   0   0   0   0   \n",
       "2   0   1   0   0   0   0   0   1   0   0  ...   0   0   0   0   0   1   0   \n",
       "3   1   0   1   1   0   1   1   0   0   0  ...   0   0   0   0   1   0   0   \n",
       "4   0   0   0   0   0   0   0   0   1   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   92  93  94  \n",
       "0   0   0   1  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   1   1   0  \n",
       "4   0   0   0  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(X.toarray())\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get TFIDF matrix as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208745</td>\n",
       "      <td>0.208745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.132673</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.317869  0.000000  0.000000   \n",
       "2  0.000000  0.222236  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.132673  0.000000  0.132673  0.132673  0.000000  0.132673  0.132673   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7         8         9   ...        85        86        87        88  \\\n",
       "0  0.000000  0.000000  0.208745  ...  0.208745  0.000000  0.208745  0.208745   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.317869  0.000000  0.000000   \n",
       "2  0.222236  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.428411  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         89        90        91        92        93        94  \n",
       "0  0.000000  0.000000  0.208745  0.000000  0.000000  0.208745  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.222236  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.132673  0.000000  0.000000  0.132673  0.132673  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfTransform = TfidfTransformer()\n",
    "X_tfidf = tfidfTransform.fit_transform(X)\n",
    "X_tfidf = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tfidf = X_tfidf.T\n",
    "DT_tfidf = np.array(DT_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.53396784 25.476927   23.90741835 21.10656234  2.97512447]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=5)\n",
    "DTtrans = pca.fit(DT_tfidf).transform(DT_tfidf)\n",
    "varPercentage = pca.explained_variance_ratio_*100\n",
    "print(varPercentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(1, 6), varPercentage, marker='^')\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ComplementNB()\n",
    "'''\n",
    "Typically out performs Multinomial Naive Bayes with text data. Hyperparameter to tune is alpha (Laplace smoothing). \n",
    "The default = 1 (new tokens in the test data that were not in the training data are ignored). A large alpha pushes the token to 50/50\n",
    "which is not useful information.\n",
    "If our comments were longer text we could use the second normalization.\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter dictionaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* max_df: ignore terms that have a document frquency strictly higher than the given threshold  \n",
    "* min_df: ignore terms that have a document frequency strictly lower than the given threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a selected combination from Stage 1(Preprocessing) & Stage 2(Sampling).\n",
    "\n",
    "# Tuning Stage 4 unable to complete with Naive Bayes as cannot pass negative values to NB\n",
    "\n",
    "# Stage 3 hyperparameter tunning\n",
    "\n",
    "# Hyperparameter tunning\n",
    "\n",
    "# parameters for CountVectorizer()\n",
    "countvect_params = {\n",
    "    'count__min_df': [1, 3, 10], \n",
    "    'count__max_df': [0.6, 0.8, 0.9]\n",
    "    \n",
    "}\n",
    "\n",
    "# parameters for TfidfVectorizer()\n",
    "vectorizer_params = {\n",
    "    'vect__min_df': [1, 3, 10], \n",
    "    'vect__max_df': [0.6, 0.8, 0.9],\n",
    "    'vect__norm': ['l1','l2']\n",
    "}\n",
    "\n",
    "# parameters for Complimentary Naive Bayes()\n",
    "compNB_params = {\n",
    "    'alpha' : [1, 10, 100]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a selected combination from Stage 1(Preprocessing) & Stage 2(Sampling).\n",
    "# Tuning Stage 4 (best number of dimensions occurs after Stage 1 & 2)\n",
    "\n",
    "# Stage 3 & 4 tunning and hyperparameter tunning\n",
    "\n",
    "#Hyperparameter tunning\n",
    "\n",
    "#Token \n",
    "min_DF = [1, 3, 10]\n",
    "max_DF = [0.6, 0.8, 0.9]\n",
    "#norm = ['l1','l2']\n",
    "\n",
    "#Dimensionality Recudtion\n",
    "n_components = [1,2,3,4]\n",
    "\n",
    "#Classifer \n",
    "n_neighbors = [5, 10]\n",
    "\n",
    "#Parameter dictionary for KNN Pipeline\n",
    "KNN_params =[\n",
    "    {\n",
    "        'token_value': [CountVectorizer(), TfidfVectorizer()],\n",
    "        'token_value__min_df': min_DF,\n",
    "        'token_value__max_df': max_DF,\n",
    "        'reduce_dim': ['passthrough'],\n",
    "        'clf__n_neighbors': n_neighbors\n",
    "    },\n",
    "    {\n",
    "        'token_value': [CountVectorizer(), TfidfVectorizer()],\n",
    "        'token_value__min_df': min_DF,\n",
    "        'token_value__max_df': max_DF,\n",
    "        'reduce_dim': [TruncatedSVD()],\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'clf__n_neighbors': n_neighbors\n",
    "    }\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rocchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is a selected combination from Stage 1(Preprocessing) & Stage 2(Sampling).\n",
    "# Tuning Stage 4 (best number of dimensions occurs after Stage 1 & 2)\n",
    "\n",
    "# Stage 3 & 4 tunning and hyperparameter tunning\n",
    "\n",
    "#Hyperparameter tunning\n",
    "\n",
    "#Token \n",
    "min_DF = [1, 3, 10]\n",
    "max_DF = [0.6, 0.8, 0.9]\n",
    "#norm = ['l1','l2']\n",
    "\n",
    "#Dimensionality Recudtion\n",
    "n_components = [2,5]\n",
    "\n",
    "#Classifer \n",
    "metric = ['euclidian', 'cosine']\n",
    "\n",
    "#Parameter dictionary for KNN Pipeline\n",
    "Rocchio_params =[\n",
    "    {\n",
    "        'token_value': [CountVectorizer(), TfidfVectorizer()],\n",
    "        'token_value__min_df': min_DF,\n",
    "        'token_value__max_df': max_DF,\n",
    "        'reduce_dim': ['passthrough'],\n",
    "        'clf__metric': metric\n",
    "    },\n",
    "    {\n",
    "        'token_value': [CountVectorizer(), TfidfVectorizer()],\n",
    "        'token_value__min_df': min_DF,\n",
    "        'token_value__max_df': max_DF,\n",
    "        'reduce_dim': [TruncatedSVD()],\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'clf__metric': metric\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipeline using term counts and complimentary Naive Bayes\n",
    "pipe_CountVect_CompNB = Pipeline([\n",
    "    ('count', CountVectorizer()),\n",
    "    ('clf', ComplementNB())\n",
    "    ])\n",
    "\n",
    "# pipeline using Tfidf and complimentary Naive Bayes\n",
    "pipe_TfidfVect_CompNB = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    (\"clf\", ComplementNB())\n",
    "    ])\n",
    "\n",
    "# KNN Pipeline\n",
    "pipe_KNN = Pipeline(\n",
    "    [\n",
    "        ('token_value', 'passthrough'),\n",
    "        ('reduce_dim', 'passthrough'),\n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Rocchio Pipeline\n",
    "pipe_Rocchio = Pipeline(\n",
    "    [\n",
    "        ('token_value', 'passthrough'),\n",
    "        ('reduce_dim', 'passthrough'),\n",
    "        ('clf', NearestCentroid())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of different subsets of the data\n",
    "subsets = {\n",
    "    'subset1': [train_df_subset, train_lab_subset, test_df_subset, test_lab_subset],\n",
    "        # filter: only 1000 instances per df\n",
    "        # filter: only 'toxic' labels used\n",
    "    #'subset2': [train_df_subset2, train_lab_subset2, test_df_subset2, test_lab_subset2],\n",
    "        # filter: labels with -1 were removed from test set\n",
    "        # filter: only 'toxic' labels used\n",
    "    #'subset3': [train_df_subset3, train_lab_subset3, test_df_subset3, test_lab_subset3],\n",
    "        # filter : only 1000 instances per df\n",
    "        # filter : toxicity levels as labels used\n",
    "}\n",
    "\n",
    "#list of parameter dictionaries\n",
    "parameters = [\n",
    "    countvect_params,\n",
    "    vectorizer_params,\n",
    "    KNN_params,\n",
    "    Rocchio_params\n",
    "    ]\n",
    "\n",
    "#list of different pipelines\n",
    "pipelines = [\n",
    "    pipe_CountVect_CompNB,\n",
    "    pipe_TfidfVect_CompNB,\n",
    "    pipe_KNN,\n",
    "    pipe_Rocchio\n",
    "    ]\n",
    "\n",
    "#pipeline with parameter dictionary\n",
    "pipe_parm_dict = {\n",
    "    'CountVect_CompNB': [pipe_CountVect_CompNB, countvect_params],\n",
    "    'TfidfVect_CompNB': [pipe_TfidfVect_CompNB, vectorizer_params],\n",
    "    'KNN_pipeline': [pipe_KNN, KNN_params],\n",
    "    'Rocchio_pipeline': [pipe_Rocchio, Rocchio_params]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_subset(pipe_params, subset):\n",
    "    'perform the pipeline(transform curpus to matrix and use to make model, use on test data) using the parameter dictionary on the data'\n",
    "\n",
    "    #set variables from input\n",
    "    pipe, parameters = pipe_params\n",
    "    train_data, train_labels, test_data, test_labels = subset\n",
    "    \n",
    "    def gridSearch(pipe, parameters):\n",
    "        'runs the randomized search CV function with input pipe and parameters'\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator = pipe,\n",
    "            param_grid = parameters,\n",
    "            n_jobs = 2,  \n",
    "        )\n",
    "        return grid_search\n",
    "    \n",
    "    #set Randomized Search CV with input pipe and parameters\n",
    "    grid_search = gridSearch(pipe, parameters)\n",
    "    #fit the data to the model through the pipe and randomized CV\n",
    "    grid_search.fit(train_data,train_labels)\n",
    "\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    print('Pipeline = {}'.format(pipe))\n",
    "    \n",
    "    #formating print statments\n",
    "    print('Best parameters = ')\n",
    "    bestparams = []\n",
    "    if type(parameters) is list:\n",
    "        for param_dict in parameters:\n",
    "            if type(param_dict) is dict:\n",
    "                for param_name in sorted(param_dict.keys()):\n",
    "                    if param_name in best_parameters.keys():\n",
    "                        x = str(param_name) + ': ' + str(best_parameters[param_name])\n",
    "                        if x not in bestparams:\n",
    "                            bestparams.append(x)\n",
    "        print(*bestparams, sep = '\\n')\n",
    "                #print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "    else:\n",
    "        for param in sorted(parameters.keys()):\n",
    "            print(f\"{param}: {best_parameters[param]}\")\n",
    "                        \n",
    "\n",
    "    test_accuracy = grid_search.score(test_data, test_labels)\n",
    "    print(\n",
    "        \"Accuracy of the best parameters using the inner CV of \"\n",
    "        f\"the random search: {grid_search.best_score_:.3f}\"\n",
    "    )\n",
    "    print(f\"Accuracy on test set: {test_accuracy:.3f}\")\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model tuning on sample subset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline = Pipeline(steps=[('count', CountVectorizer()), ('clf', ComplementNB())])\n",
      "Best parameters = \n",
      "count__max_df: 0.6\n",
      "count__min_df: 1\n",
      "Accuracy of the best parameters using the inner CV of the random search: 0.902\n",
      "Accuracy on test set: 0.496\n"
     ]
    }
   ],
   "source": [
    "run_on_subset(pipe_parm_dict['CountVect_CompNB'], subsets['subset1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline = Pipeline(steps=[('vect', TfidfVectorizer()), ('clf', ComplementNB())])\n",
      "Best parameters = \n",
      "vect__max_df: 0.6\n",
      "vect__min_df: 1\n",
      "vect__norm: l1\n",
      "Accuracy of the best parameters using the inner CV of the random search: 0.905\n",
      "Accuracy on test set: 0.458\n"
     ]
    }
   ],
   "source": [
    "run_on_subset(pipe_parm_dict['TfidfVect_CompNB'], subsets['subset1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline = Pipeline(steps=[('token_value', 'passthrough'), ('reduce_dim', 'passthrough'),\n",
      "                ('clf', KNeighborsClassifier())])\n",
      "Best parameters = \n",
      "clf__n_neighbors: 5\n",
      "reduce_dim: passthrough\n",
      "token_value: TfidfVectorizer(max_df=0.6)\n",
      "token_value__max_df: 0.6\n",
      "token_value__min_df: 1\n",
      "Accuracy of the best parameters using the inner CV of the random search: 0.920\n",
      "Accuracy on test set: 0.478\n"
     ]
    }
   ],
   "source": [
    "run_on_subset(pipe_parm_dict['KNN_pipeline'], subsets['subset1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jjbocek\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [  nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan 0.869 0.855 0.821 0.869 0.855 0.821\n",
      " 0.869 0.855 0.821 0.913 0.908 0.885 0.913 0.908 0.885 0.913 0.908 0.885\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
      " 0.37  0.443 0.532 0.373 0.443 0.534 0.374 0.444 0.534 0.443 0.435 0.483\n",
      " 0.438 0.44  0.48  0.443 0.437 0.484 0.529 0.57  0.58  0.53  0.568 0.582\n",
      " 0.529 0.569 0.578 0.846 0.878 0.896 0.822 0.843 0.895 0.821 0.876 0.89 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\jjbocek\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_nearest_centroid.py:164: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline = Pipeline(steps=[('token_value', 'passthrough'), ('reduce_dim', 'passthrough'),\n",
      "                ('clf', NearestCentroid())])\n",
      "Best parameters = \n",
      "clf__metric: cosine\n",
      "reduce_dim: passthrough\n",
      "token_value: TfidfVectorizer(max_df=0.6)\n",
      "token_value__max_df: 0.6\n",
      "token_value__min_df: 1\n",
      "Accuracy of the best parameters using the inner CV of the random search: 0.913\n",
      "Accuracy on test set: 0.522\n"
     ]
    }
   ],
   "source": [
    "run_on_subset(pipe_parm_dict['Rocchio_pipeline'], subsets['subset1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_subset(pipe_parm_dict['CountVect_CompNB'], subsets['subset3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_subset(pipe_parm_dict['TfidfVect_CompNB'], subsets['subset3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_subset(pipe_parm_dict['KNN_pipeline'], subsets['subset3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_subset(pipe_parm_dict['Rocchio_pipeline'], subsets['subset3'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d8c354d3cf95362cb56a9fc61661589ba752411f03c7f5d3fd3d8348f563371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
