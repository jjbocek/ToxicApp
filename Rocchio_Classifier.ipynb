{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rocchio Classifier\n",
    "\n",
    "This notebook contains all methods related to creating and tuning the model for the Rocchio Classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A clean_data.p file was created to store all of the clean data from our training data set.  Below, the clean data is read and uploaded to pandas dataframes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "%run Doc2Vec.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4398, 5)\n",
      "(1516, 5)\n"
     ]
    }
   ],
   "source": [
    "file_object = open('clean_data1.p', 'rb')\n",
    "clean_data = pickle.load(file_object)\n",
    "train_ds = clean_data[0]\n",
    "test_ds = clean_data[1]\n",
    "\n",
    "print(train_ds.shape)\n",
    "print(test_ds.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "%run Doc2Vec.ipynb\n",
    "# retrieve the comments only\n",
    "train_comments_df = train_ds['comment_text']\n",
    "test_comments_df = test_ds['comment_text']\n",
    "d2v_model = doc2vec.Doc2Vec(vector_size = 1000, min_count = 5, epochs = 40)\n",
    "\n",
    "train_vectors = get_doc2vec_vectors(d2v_model, train_comments_df)\n",
    "\n",
    " # #using default values for now\n",
    "tokenized_test_comments = tokenize_comments(test_comments_df)\n",
    "test_vectors = infer_vectors(d2v_model, tokenized_test_comments, \"\")\n",
    "\n",
    "#store the test labels\n",
    "test_labels = test_ds['toxicity_level']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Methods for Custom Rocchio Classifier\n",
    "\n",
    "Below are methods to use the Rocchio Classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_number_of_classes(classes):\n",
    "    return len(np.unique(classes))\n",
    "\n",
    "def compute_tf_idf_matrix(trainingData):\n",
    "    # compute term frequencies to get an idea of their distributions across the corpus.\n",
    "    termFreq = trainingData.sum(axis=1)\n",
    "    # print(\"termFreq:\" + str(termFreq))\n",
    "\n",
    "    # computer the doc frequency, the number of docs in which the term appears divided by total number of docs\n",
    "    # doc counts for each term\n",
    "    docFreq = pd.DataFrame([(trainingData != 0).sum(1)]).T  # if TD!=0 add 1 and sum it up across\n",
    "    # print(\"docFreq:\" + str(docFreq))\n",
    "\n",
    "    # Create a matrix with all entries = number of docs\n",
    "    numDocs = trainingData.shape[1]\n",
    "    newMatrix = np.ones(np.shape(trainingData), dtype=float) * numDocs\n",
    "    np.set_printoptions(precision=2, suppress=True, linewidth=120)\n",
    "    # print(\"newMatrix\" + str(newMatrix))\n",
    "\n",
    "    # Convert each entry into IDF values\n",
    "    # IDF is the log of the inverse of document frequency\n",
    "    # Note that IDF is only a function of the term, so all columns will be identical.\n",
    "    idf = np.log2(np.divide(newMatrix, np.array(docFreq)))\n",
    "    # print(\"idf\" + str(idf))\n",
    "\n",
    "    # Finally compute the TFxIDF values for each document-term entry\n",
    "    tf_idf = trainingData * idf\n",
    "    pd.set_option(\"display.precision\", 2)\n",
    "    # print(\"tf_idf\" + str(tf_idf))\n",
    "    return tf_idf\n",
    "\n",
    "\"\"\"A Training Function for the Rocchio Classification Algorithm\n",
    ":param data: an np array of the training data formatted as a doc-term frequency matrix,\n",
    "a ttd*idf matrix or else a doc2vec vector matrix\n",
    ":param labels: an np array of the corresponding labels for the data\n",
    ":returns: the prototype vectors for each class (a.k.a. label)\n",
    "\"\"\"\n",
    "def rocchio_train(train, labels):\n",
    "    # the prototype will be a dictionary with unique class labels as keys and on dimensional arrays representing\n",
    "    # the prototype.\n",
    "    prototype = {}\n",
    "    #tf_idf_train = compute_tf_idf_matrix(train)\n",
    "    num_labels = get_number_of_classes(labels)\n",
    "    #print(\"The number of classes:\" + str(num_labels))\n",
    "\n",
    "    # add each class to the prototype dictionary\n",
    "    # the class is the key\n",
    "    # a one dimensional array equal to the length of a term vector array is initialized with all 0s\n",
    "    for label in labels:\n",
    "        term_vector_array = np.zeros(np.shape(len(train[0])), dtype=float)\n",
    "        prototype[label] = term_vector_array\n",
    "\n",
    "    length = len(train)\n",
    "    for i in range(length):\n",
    "        # first figure out which class this entry belongs to\n",
    "        label = labels[i]\n",
    "\n",
    "        # next sum up the entire document term vector\n",
    "        prototype[label] = prototype.get(label) + np.array(train[i])\n",
    "\n",
    "    return prototype\n",
    "\n",
    "\"\"\"A Classifier Function for the Rocchio Classification Algorithm\n",
    ":param data: a dictionary of prototype vectors where the class is a key and the value is the vector\n",
    "for the corresponding class.\n",
    ":param instance: the test instance to classify\n",
    ":returns: the predicted class\n",
    "\"\"\"\n",
    "def rocchio_classifier(prototype, test_instance):\n",
    "    m = -2\n",
    "    predicted_class = -1  # intialize the predicted class to -1 by default\n",
    "    for classLabel in prototype:\n",
    "        # (compute similarity to prototype vector)\n",
    "        # use cosine similarity\n",
    "        # cosine distance is 1 - (dot product / L2 norm)\n",
    "        term_vector = prototype[classLabel]\n",
    "        prototype_norm = np.array([np.linalg.norm(term_vector) for i in range(len(term_vector))])\n",
    "        test_instance_norm = np.linalg.norm(test_instance)\n",
    "\n",
    "        sims = np.dot(term_vector, test_instance) / (prototype_norm * test_instance_norm)\n",
    "\n",
    "        # get the maximum cosSim\n",
    "        max_cos_sim = np.max(sims)\n",
    "\n",
    "        if max_cos_sim > m:\n",
    "            m = max_cos_sim\n",
    "            predicted_class = classLabel\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "\"\"\"An Evaluation Function for the Rocchio Classification Algorithm\n",
    ":param test_data: an np array of the test data formatted as a doc-term frequency matrix,\n",
    "a ttd*idf matrix or else a doc2vec vector matrix\n",
    ":param test_labels: an np array of the corresponding labels for the data\n",
    ":param prototype: a dictionary of prototype vectors to use for classification\n",
    ":returns: the accuracy of the model\n",
    "\"\"\"\n",
    "def rocchio_evaluate(test_data, test_labels, prototype):\n",
    "    num_correct = 0\n",
    "\n",
    "    # iterate through the test instances and call the rocchio classifier\n",
    "    test_data_len = len(test_data)\n",
    "    for i in range(test_data_len):\n",
    "        predicted_label = rocchio_classifier(prototype, test_data[i])\n",
    "        test_label = test_labels[i]\n",
    "        #print(\"Predicted Label:\" + str(predicted_label) + \" \" + \"Test Label:\" + str(test_labels[i]))\n",
    "        if predicted_label == test_label:\n",
    "            num_correct = num_correct + 1\n",
    "    accuracy = (num_correct / test_data_len * 100.0)\n",
    "    print(\"Rocchio classifier - Number of test instances classified correctly:\" + str(\n",
    "        num_correct) + \" Percent Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Custom Rochio Implementation using Doc2Vec vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocchio classifier - Number of test instances classified correctly:960 Percent Accuracy: 63.3245382585752\n"
     ]
    }
   ],
   "source": [
    "# create np arrays for inputs\n",
    "train_vectors = np.array(train_vectors)\n",
    "train_labels = np.array(train_ds['toxicity_level'])\n",
    "\n",
    "# generate the prototype vectors\n",
    "prototype_vectors = rocchio_train(train_vectors, train_labels)\n",
    "\n",
    "#classify test data using prototype vectors\n",
    "test_vectors = np.array(test_vectors)\n",
    "test_labels = np.array(test_labels)\n",
    "rocchio_evaluate(test_vectors, test_labels, prototype_vectors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Running Nearest Centroid from sklearn using Doc2Vec vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Methods for Nearest Centroid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "\"\"\"An Classifier Function for the Nearest Centroid Algorithm\n",
    ":param train: an np array of the train data formatted as a doc-term frequency matrix,\n",
    "a ttd*idf matrix or else a doc2vec vector matrix\n",
    ":param train: an np array of the corresponding labels for the train data\n",
    ":param test_data: an np array of the test data formatted as a doc-term frequency matrix,\n",
    "a ttd*idf matrix or else a doc2vec vector matrix\n",
    ":param test_labels: an np array of the corresponding labels for the test data\n",
    ":returns: the accuracy of the model\n",
    "\"\"\"\n",
    "def rocchio_classifier_nearest_centroid(train, train_labels, test_data, test_labels):\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(train, train_labels)\n",
    "    predictions = clf.predict(test_data)\n",
    "\n",
    "    numCorrect = 0\n",
    "    test_data_len = len(test_data)\n",
    "    for i in range(test_data_len):\n",
    "        if predictions[i] == test_labels[i]:\n",
    "            numCorrect = numCorrect + 1\n",
    "\n",
    "    accuracy = (numCorrect / test_data_len * 100.0)\n",
    "    print(\"Nearest Centroid classifier - Number of test instances classified correctly:\" + str(\n",
    "        numCorrect) + \" Percent Accuracy: \" + str(accuracy))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Centroid classifier - Number of test instances classified correctly:836 Percent Accuracy: 55.145118733509236\n"
     ]
    }
   ],
   "source": [
    "rocchio_classifier_nearest_centroid(train_vectors, train_labels, test_vectors, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Doc2Vec Parameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dm': 0, 'vector_size': 500, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 500, 'window': 5, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "dm = [0]\n",
    "vector_size = [500, 1000]\n",
    "window = [2,5]\n",
    "hs = [1]\n",
    "paramsList = [{'dm': item[0],\n",
    "               'vector_size': item[1],\n",
    "               'window': item[2],\n",
    "               'hs': 1,\n",
    "               'negative': 0\n",
    "               } for item in\n",
    "                 list(itertools.product(*[dm,\n",
    "                                          vector_size,\n",
    "                                          window,\n",
    "                                          hs]))\n",
    "              ]\n",
    "\n",
    "print(paramsList)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating {'dm': 0, 'vector_size': 500, 'window': 2, 'hs': 1, 'negative': 0}\n",
      "Nearest Centroid classifier - Number of test instances classified correctly:957 Percent Accuracy: 63.12664907651715\n",
      "Evaluating {'dm': 0, 'vector_size': 500, 'window': 5, 'hs': 1, 'negative': 0}\n",
      "Nearest Centroid classifier - Number of test instances classified correctly:955 Percent Accuracy: 62.99472295514512\n",
      "Evaluating {'dm': 0, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}\n",
      "Nearest Centroid classifier - Number of test instances classified correctly:959 Percent Accuracy: 63.25857519788918\n",
      "Evaluating {'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}\n",
      "Nearest Centroid classifier - Number of test instances classified correctly:958 Percent Accuracy: 63.19261213720316\n"
     ]
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%run Doc2Vec.ipynb\n",
    "\n",
    "file_object = open('clean_data1.p', 'rb')\n",
    "clean_data = pickle.load(file_object)\n",
    "train_ds = clean_data[0]\n",
    "test_ds = clean_data[1]\n",
    "\n",
    "#pre-process the corpus\n",
    "train_comments_df = train_ds['comment_text']\n",
    "test_comments_df = test_ds['comment_text']\n",
    "train_labels_df = train_ds['toxicity_level']\n",
    "test_labels_df = test_ds['toxicity_level']\n",
    "\n",
    "def evaluateDoc2VecParams():\n",
    "    # Tag docs\n",
    "    train_tokenized_comments = tokenize_comments(train_ds['comment_text'])\n",
    "    train_tagged_documents = get_tagged_documents(train_tokenized_comments)\n",
    "    scoreList = []\n",
    "    for param in paramsList:\n",
    "      print(\"Evaluating \"+str(param))\n",
    "      try:\n",
    "        d2v_model = doc2vec.Doc2Vec(train_tagged_documents,\n",
    "                        dm=param['dm'],\n",
    "                        vector_size=param['vector_size'],\n",
    "                        window=param['window'],\n",
    "                        min_count=1,\n",
    "                        epochs=10,\n",
    "                        hs=param['hs'],\n",
    "                        seed=516)\n",
    "        train_vectors = get_doc2vec_vectors(d2v_model,train_comments_df)\n",
    "\n",
    "        tokenized_test_comments = tokenize_comments(test_comments_df)\n",
    "        test_vectors = infer_vectors(d2v_model, tokenized_test_comments, \"\")\n",
    "\n",
    "        train_labels = np.array(train_labels_df)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        test_labels = np.array(test_labels_df)\n",
    "\n",
    "        rocchio_classifier_nearest_centroid(train_vectors, train_labels, test_vectors, test_labels)\n",
    "        # generate the prototype vectors\n",
    "        #prototype_vectors = rocchio_train(train_vectors, train_labels)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        #rocchio_evaluate(test_vectors, test_labels, prototype_vectors)\n",
    "      except Exception as error:\n",
    "        print(f'Cannot evaluate model with parameters {param} because of error: {error}')\n",
    "        continue\n",
    "    return scoreList\n",
    "\n",
    "evaluateDoc2VecParams()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Pipeline to Tune Doc2Vec (these didn't work for me)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter doc2vec for estimator Pipeline(steps=[('token_value',\n                 <gensim.models.doc2vec.Doc2Vec object at 0x7fd2b95762b0>),\n                ('reduce_dim', 'passthrough'), ('clf', NearestCentroid())]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31m_RemoteTraceback\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;31m_RemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n    return self.function(*args, **kwargs)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 668, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 188, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 245, in set_params\n    raise ValueError(\nValueError: Invalid parameter doc2vec for estimator Pipeline(steps=[('token_value',\n                 <gensim.models.doc2vec.Doc2Vec object at 0x7fd2b95762b0>),\n                ('reduce_dim', 'passthrough'), ('clf', NearestCentroid())]). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_3063/495258760.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m                         n_jobs=2)\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m \u001B[0mfitted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrocchio_grid\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_comments\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;31m# Best parameters\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    889\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    890\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 891\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    892\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    893\u001B[0m             \u001B[0;31m# multimetric is determined here because in the case of a callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36m_run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1390\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_run_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1391\u001B[0m         \u001B[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1392\u001B[0;31m         \u001B[0mevaluate_candidates\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mParameterGrid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparam_grid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1393\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1394\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mevaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    836\u001B[0m                     )\n\u001B[1;32m    837\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 838\u001B[0;31m                 out = parallel(\n\u001B[0m\u001B[1;32m    839\u001B[0m                     delayed(_fit_and_score)(\n\u001B[1;32m    840\u001B[0m                         \u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbase_estimator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1055\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1056\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1057\u001B[0m             \u001B[0;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1058\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    933\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    934\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'supports_timeout'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 935\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    936\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    937\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mwrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    540\u001B[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001B[1;32m    541\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 542\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfuture\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    543\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mCfTimeoutError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    544\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001B[0m in \u001B[0;36mresult\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    444\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mCancelledError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    445\u001B[0m                 \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mFINISHED\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 446\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__get_result\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    447\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    448\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\u001B[0m in \u001B[0;36m__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    389\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_exception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 391\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_exception\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    392\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m                 \u001B[0;31m# Break a reference cycle with the exception in self._exception\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Invalid parameter doc2vec for estimator Pipeline(steps=[('token_value',\n                 <gensim.models.doc2vec.Doc2Vec object at 0x7fd2b95762b0>),\n                ('reduce_dim', 'passthrough'), ('clf', NearestCentroid())]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train_comments = np.array(train_ds['comment_text'])\n",
    "train_labels = np.array(train_ds['toxicity_level'])\n",
    "\n",
    "param_grid = {'doc2vec__vector_size': [1000, 5000, 10000],\n",
    "              'doc2vec__min_count': [2, 5, 10],\n",
    "              'doc2vec__epochs': [40, 400],\n",
    "}\n",
    "\n",
    "#pipe_rocchio = Pipeline([('doc2vec', doc2vec.Doc2Vec()), ('rocchio', NearestCentroid())])\n",
    "Rocchio_params =[\n",
    "    {\n",
    "        'token_value': [doc2vec.Doc2Vec()],\n",
    "        'doc2vec__vector_size': [1000, 5000, 10000],\n",
    "        'doc2vec__min_count': [2, 5, 10],\n",
    "        'reduce_dim': ['passthrough'],\n",
    "        'clf__metric': ['euclidian', 'cosine']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Rocchio Pipeline\n",
    "pipe_Rocchio = Pipeline(\n",
    "    [\n",
    "        ('token_value', 'passthrough'),\n",
    "        ('reduce_dim', 'passthrough'),\n",
    "        ('clf', NearestCentroid())\n",
    "    ]\n",
    ")\n",
    "\n",
    "rocchio_grid = GridSearchCV(pipe_Rocchio,\n",
    "                        param_grid=Rocchio_params,\n",
    "                        scoring=\"accuracy\",\n",
    "                        verbose=3,\n",
    "                        n_jobs=2)\n",
    "\n",
    "fitted = rocchio_grid.fit(train_comments,train_labels)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters: {}\\n\".format(rocchio_grid.best_params_))\n",
    "print(\"Best accuracy: {}\\n\".format(rocchio_grid.best_score_))\n",
    "print(\"Finished.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV 1/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=0, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=0, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=1, doc2vec__size=100, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=2;, score=nan total time=   0.0s\n",
      "[CV 1/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 2/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 3/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 4/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n",
      "[CV 5/5] END doc2vec__dm=1, doc2vec__size=200, doc2vec__window=3;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_3063/2563129205.py\", line 57, in fit_transform\n",
      "    self.fit(raw_documents)\n",
      "  File \"/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_3063/2563129205.py\", line 39, in fit\n",
      "    self.d2v_model = Doc2Vec(size=self.size, window=self.window, dm=self.dm, iter=5, alpha=0.025, min_alpha=0.001)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/gensim/models/doc2vec.py\", line 294, in __init__\n",
      "    super(Doc2Vec, self).__init__(\n",
      "TypeError: __init__() got an unexpected keyword argument 'size'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_3063/2563129205.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     72\u001B[0m                         n_jobs=1)\n\u001B[1;32m     73\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m \u001B[0mfitted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlog_grid\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'comment_text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_ds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'toxicity_level'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[0;31m# Best parameters\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    924\u001B[0m             \u001B[0mrefit_start_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    925\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 926\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    927\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    928\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    388\u001B[0m         \"\"\"\n\u001B[1;32m    389\u001B[0m         \u001B[0mfit_params_steps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_fit_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 390\u001B[0;31m         \u001B[0mXt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params_steps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    391\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0m_print_elapsed_time\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Pipeline\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_message\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    392\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_final_estimator\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"passthrough\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, X, y, **fit_params_steps)\u001B[0m\n\u001B[1;32m    346\u001B[0m                 \u001B[0mcloned_transformer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    347\u001B[0m             \u001B[0;31m# Fit or load from cache the current transformer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 348\u001B[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001B[0m\u001B[1;32m    349\u001B[0m                 \u001B[0mcloned_transformer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    350\u001B[0m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/memory.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    348\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 349\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    350\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    351\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcall_and_shelve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001B[0m\n\u001B[1;32m    891\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0m_print_elapsed_time\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage_clsname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    892\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"fit_transform\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 893\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    894\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    895\u001B[0m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_3063/2563129205.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mraw_documents\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_3063/2563129205.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0;31m# Initialize model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0mdoc2vec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDoc2Vec\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0md2v_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDoc2Vec\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwindow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwindow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.025\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_alpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.001\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m         \u001B[0;31m# Tag docs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0mtokenized_comments\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDoc2Vec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize_comments\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'comment_text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/doc2vec.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001B[0m\n\u001B[1;32m    292\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvectors_lockf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mREAL\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 294\u001B[0;31m         super(Doc2Vec, self).__init__(\n\u001B[0m\u001B[1;32m    295\u001B[0m             \u001B[0msentences\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    296\u001B[0m             \u001B[0mcorpus_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "%run Doc2Vec.ipynb\n",
    "\n",
    "\n",
    "\n",
    "train_comments = np.array(train_ds['comment_text'])\n",
    "train_labels = np.array(train_ds['toxicity_level'])\n",
    "\n",
    "class Doc2VecModel(BaseEstimator):\n",
    "\n",
    "    def __init__(self, size=1, window=1, dm=1):\n",
    "        self.d2v_model = None\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.dm = dm\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        # Initialize model\n",
    "        doc2vec.Doc2Vec\n",
    "        self.d2v_model = Doc2Vec(size=self.size, window=self.window, dm=self.dm, iter=5, alpha=0.025, min_alpha=0.001)\n",
    "        # Tag docs\n",
    "        tokenized_comments = Doc2Vec.tokenize_comments(train_ds['comment_text'])\n",
    "        tagged_documents = Doc2Vec.get_tagged_documents(tokenized_comments)\n",
    "        # Build vocabulary\n",
    "        self.d2v_model.build_vocab(tagged_documents)\n",
    "        # Train model\n",
    "        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.d2v_model.iter)\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        X = []\n",
    "        for index, row in raw_documents.iteritems():\n",
    "            X.append(self.d2v_model.infer_vector(row))\n",
    "        X = pd.DataFrame(X, index=raw_documents.index)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)\n",
    "\n",
    "\n",
    "param_grid = {'doc2vec__window': [2, 3],\n",
    "              'doc2vec__dm': [0,1],\n",
    "              'doc2vec__size': [100,200]\n",
    "}\n",
    "\n",
    "pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('log', LogisticRegression())])\n",
    "\n",
    "log_grid = GridSearchCV(pipe_log,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring=\"accuracy\",\n",
    "                        verbose=3,\n",
    "                        n_jobs=1)\n",
    "\n",
    "fitted = log_grid.fit(train_ds['comment_text'], train_ds['toxicity_level'])\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters: {}\\n\".format(log_grid.best_params_))\n",
    "print(\"Best accuracy: {}\\n\".format(log_grid.best_score_))\n",
    "print(\"Finished.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Doc2Vec Parameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dm': 0, 'vector_size': 500, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 500, 'window': 5, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "dm = [0]\n",
    "vector_size = [500, 1000]\n",
    "window = [2,5]\n",
    "hs = [1]\n",
    "paramsList = [{'dm': item[0],\n",
    "               'vector_size': item[1],\n",
    "               'window': item[2],\n",
    "               'hs': 1,\n",
    "               'negative': 0\n",
    "               } for item in\n",
    "                 list(itertools.product(*[dm,\n",
    "                                          vector_size,\n",
    "                                          window,\n",
    "                                          hs]))\n",
    "              ]\n",
    "\n",
    "print(paramsList)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
