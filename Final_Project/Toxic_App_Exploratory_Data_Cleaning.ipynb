{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic App - DSC478 Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors: *Jeffrey Bocek, Xuyang Ji & Anna-Lisa Vu*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview \n",
    "\n",
    "Unfortunately, having conversation about topics one cares about can be challenging under some scenerios, such as the threat of online abuse and harassment. Those insecure online environment not only causes many individuals to refrain from expressing themselves and seeking diverse opinions; but also has led to various platforms struggling to effectively facilitate discussions, resulting in many communities limiting or forcing shutting down user comment sections.\n",
    "\n",
    "\n",
    "With a goal to foster healthier online communities by addressing the issue, our team worked on developing a Python application focuses on comment toxicity detection. The app can be used as a third-party library or extension for social media sites or public sites where users are allowed to leave comments. With various clustering and predictive models stored in the backend, the app allows users to detect the toxicity level of specific queries, and revise them for maintaining a more respectful online community. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "The provided dataset contains a large number of Wikipedia comments which have been labeled by human raters for toxic behavior, such as toxic, severe toxic, obscene, threat, insult and identity hate. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import random\n",
    "import operator #for sorting \n",
    "from sklearn import preprocessing # for normalization \n",
    "from collections import Counter #finding the majority \n",
    "\n",
    "#Text Preprocessing \n",
    "import re # for number removal \n",
    "import string # for punctutation removal \n",
    "\n",
    "import nltk \n",
    "## for stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords= stopwords.words('english')\n",
    "## lemmatization \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pickle  #save variables to file\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_comments = pd.read_csv('test.csv')\n",
    "test_lab= pd.read_csv('test_labels.csv',header=0, na_values=-1)\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.merge(test_comments, test_lab, on=\"id\")\n",
    "test_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Length \n",
    "Looking at the histoplot for comment length differences between clean and toxic comments, clean comments tend to be approximately one-fourth longer than toxic comments on average.  Upon examining random samples, it becomes apparent that many clean comments consist of long and well-crafted responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm= train_df[train_df.iloc[:,2:].sum(axis=1)==0][\"comment_text\"].str.split().apply(len)\n",
    "toxic_comm= train_df[train_df.iloc[:,2:].sum(axis=1)!=0][\"comment_text\"].str.split().apply(len)\n",
    "plt.hist([comm, toxic_comm], bins = 40, color =('c','m'), label=(\"Clean Comment\", \"Toxic Comment\"))\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Distribution for Comment Lengths\")\n",
    "plt.xlim(0,600)\n",
    "plt.xlabel(\"Word Counts\")\n",
    "plt.ylabel(\"Comment Density\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance \n",
    "The dataset comprises 159,571 comments from Wikipedia, with each comment consisting of a string data input feature and six labels that categorize the comment as toxic, severe_toxic, obscene, threat, insult, or identity_hate. The following figure shows how these labels are distributed throughout the dataset, including multilabelled data. Although there is no missing value in the training dataset; however, based on the fact that the mean values are extremely low, it can be inferred that the majority of the comments are likely to be clean/non-toxic comments. In other words, the toxicity is not evenly distributed across classes, and class imbalance is present. Upon investigation, the clean comment ratio in training set is 89.8%, while there are 58.2% of clean comments in the test set. \n",
    "\n",
    "The breakdown demonstrates that while most comments with other labels are also toxic, not all of them are. Only \"severe_toxic\" is clearly a subcategory of \"toxic,\" which is reasonable to rule out labeling errors. The observation indicates that \"toxic\" is not a overseeing label, but rather a subcategory in the bigger context with considerable overlap with other labels. Regarding the issue of multi-labelling, it would most likely to pose difficulty to train a classifier on specific labels in the raw dataset due to overlapping. The ambiguity surrounding the label assignments and the absence of clear explanations is the reason why we opted to use aggregate labels of general toxicity levels, called \"non_toxic\", \"mild_toxicity\", \"toxic\", and \"severe-toxic\" as the targets going forwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxic vs. clean comment \n",
    "clean_comm= (train_df.iloc[:,2:].sum(axis=1)==0).sum(axis=0)\n",
    "clean_test= len(test_lab[test_lab.isna().any(axis=1)])\n",
    "print(\"The clean comment ratio in training set is:\",(round(clean_comm/len(train_df),3)))\n",
    "print(\"The clean comment ratio in test set is:\",(round(clean_test/len(test_lab),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ratio of comments in each toxic class\n",
    "\n",
    "categories= list(train_df.columns.values[2:])\n",
    "                 \n",
    "counts=[]\n",
    "for category in categories:\n",
    "    count=train_df[category].sum()\n",
    "    ratio= round(count/len(train_df),3)\n",
    "    counts.append((category,count,ratio))\n",
    "category_stat = pd.DataFrame(counts, columns=[\"Class\",\"Counts\",\"Percentage\"])\n",
    "category_stat\n",
    "ax= sns.barplot(x=\"Class\",y=\"Counts\",data=category_stat)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title(\"Comments in Each Toxic Class\")\n",
    "plt.ylabel('Comment Counts')\n",
    "plt.xlabel('Comment Class ')\n",
    "ax.tick_params(labelsize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comments belong to multiple classes by getting rowSums \n",
    "multiClass_comm= train_df.iloc[:,2:].sum(axis=1).value_counts()\n",
    "multiClass= multiClass_comm[multiClass_comm.index>1]\n",
    "ax=sns.barplot(x=multiClass.index, y=multiClass.values)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title(\"Comments Belong to Multiple Classes\")\n",
    "plt.ylabel('Comment Counts')\n",
    "plt.xlabel('Classes Counts')\n",
    "ax.tick_params(labelsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "#### I. Class Resampling\n",
    "As the first step to ensuring data quality, the reconfigure_categories function transform the multi-labeled dataset into 4 separate independent classes. While the semantic meaning of toxic and severe-toxic comments shows some level of graduation; we firstly fill the all toxic column of the severe-toxic comments with 1, and extract extract all severe-toxic comments from the toxic class. The newly created non-toxic column is for comments with all target_col value of 0. Since labels are nested under each other in some cases, comments with toxicity-type defined are identified and stored in a new column called toxicity-defined. Finally, the new mild_toxicity column is created for comment with toxicity defined but not labeled as toxic/severe toxic. The key assumption is that there is no significant correlation among the following labels. \n",
    "- level 1: Non-toxic comment \n",
    "- level 2: Mild_toxicity comment\n",
    "- level 3: Toxic comment \n",
    "- level 4: severe_toxic comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "def reconfigure_categories(train_df, test_lab):\n",
    "    'add categories to df'\n",
    "\n",
    "    def plot_df_distributions(df, position, chart_title):\n",
    "        'make a bar chart of df distributions'\n",
    "        target_column =list(df.columns[position:-1])\n",
    "        labelC = df[target_column].sum()\n",
    "        plt.figure(figsize=(10,7))\n",
    "        ax = sns.barplot(x=labelC.index, y=labelC.values,dodge=False)\n",
    "        ax.set_yscale('log')\n",
    "        ax.tick_params(labelsize=7)\n",
    "        plt.title(chart_title)\n",
    "        for i in ax.containers:\n",
    "            ax.bar_label(i,)\n",
    "    \n",
    "    def config_train(train_df, position):\n",
    "        target_col = list(train_df.columns[position:])\n",
    "        train_df.loc[train_df['severe_toxic']==1,'toxic'] =1\n",
    "        train_df['non_toxic'] = 1-train_df[target_col].max(axis=1)\n",
    "        train_df['toxicity_defined'] = train_df[['insult', 'obscene', 'identity_hate', 'threat']].max(axis=1)\n",
    "        train_df['toxicity_undefined'] = 0  # initiziling all value of 0\n",
    "        # for each comment that's not toxicity defined but labelled as toxic, labeled them as 1 in new column\n",
    "        train_df.loc[(train_df['toxicity_defined'] == 0) & (train_df['toxic'] == 1), 'toxicity_undefined'] = 1\n",
    "        train_df['mild_toxicity'] = 0\n",
    "        train_df.loc[(train_df['toxicity_defined'] == 1) & (train_df['toxic'] == 0) & (train_df['severe_toxic'] == 0),\n",
    "                'mild_toxicity'] = 1\n",
    "\n",
    "        train_df['toxicity_level']=0 #initializing a column of 0\n",
    "        train_df.loc[(train_df['non_toxic']==1),'toxicity_level']=1\n",
    "        train_df.loc[(train_df['mild_toxicity']==1),'toxicity_level']=2\n",
    "        train_df.loc[(train_df['toxic']==1)& (train_df['severe_toxic']==0),'toxicity_level']=3\n",
    "        train_df.loc[(train_df['severe_toxic']==1), 'toxicity_level']=4\n",
    "\n",
    "        #drop rows with toxicity level undefined\n",
    "        train_df= train_df[train_df['toxicity_level']!= 0]\n",
    "\n",
    "        #make sure the toxic comments does not include severe toxic comments\n",
    "        train_df.loc[train_df['severe_toxic'] == 1, 'toxic'] = 0\n",
    "\n",
    "        #train_df = train_df[train_df.toxicity_level !=2]\n",
    "        #train_df['toxicity_level'] = train_df['toxicity_level'].replace([3,4], [2,3])\n",
    "        #train_df = train_df.drop(columns=['mild_toxicity'])\n",
    "        #print(train_df['toxicity_level'].value_counts())\n",
    "\n",
    "        return train_df\n",
    "\n",
    "    def config_test_lab(test_lab, position):\n",
    "        test_lab = test_lab[~test_lab.isnull().any(axis=1)]\n",
    "        target_col = list(test_lab.columns[position:])\n",
    "        #fill toxic column of sever toxic comments with 1 \n",
    "        test_lab.loc[test_lab['severe_toxic']==1,'toxic'] =1\n",
    "\n",
    "        test_lab['non_toxic'] = 1-test_lab[target_col].max(axis=1)\n",
    "        test_lab['toxicity_defined'] = test_lab[['insult', 'obscene', 'identity_hate', 'threat']].max(axis=1) \n",
    "        test_lab['toxicity_undefined'] = 0  # initiziling all value of 0\n",
    "        test_lab.loc[(test_lab['toxicity_defined'] == 0) & (test_lab['toxic'] == 1), 'toxicity_undefined'] = 1\n",
    "        test_lab['mild_toxicity'] = 0\n",
    "        test_lab.loc[(test_lab['toxicity_defined'] == 1) & (test_lab['toxic'] == 0) & (test_lab['severe_toxic'] == 0),\n",
    "                'mild_toxicity'] = 1\n",
    "\n",
    "        test_lab['toxicity_level']=0 #initializing a column of 0\n",
    "        test_lab.loc[(test_lab['non_toxic']==1),'toxicity_level']=1\n",
    "        test_lab.loc[(test_lab['mild_toxicity']==1),'toxicity_level']=2\n",
    "        test_lab.loc[(test_lab['toxic']==1)& (test_lab['severe_toxic']==0),'toxicity_level']=3\n",
    "        test_lab.loc[(test_lab['severe_toxic']==1), 'toxicity_level']=4\n",
    "\n",
    "        test_lab= test_lab[test_lab['toxicity_level']!= 0]\n",
    "\n",
    "        #make sure the toxic comments does not include severe toxic comments\n",
    "        test_lab.loc[test_lab['severe_toxic'] == 1, 'toxic'] = 0\n",
    "\n",
    "        #test_lab = test_lab[test_lab.toxicity_level !=2]\n",
    "        #test_lab['toxicity_level'] = test_lab['toxicity_level'].replace([3,4], [2,3])\n",
    "        #test_lab = test_lab.drop(columns=['mild_toxicity'])\n",
    "        #print(test_lab['toxicity_level'].value_counts())\n",
    "        return test_lab\n",
    " \n",
    "    train_df = config_train(train_df, 2)\n",
    "    test_lab = config_test_lab(test_lab, 2)\n",
    "\n",
    "    plot_df_distributions(train_df, 2, \"Training Data Class Distribution\")\n",
    "    plot_df_distributions(test_lab, 2, \"Test Data Class Distribution\")\n",
    "    \n",
    "    return train_df, test_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_lab = reconfigure_categories(train_df, test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the the tuples in non_toxic class significantly outnumbered the other classes, directly feeding the dataset with 4 classes to classifiers can make them biased in favor of the majority class. Hence, we resampled the dataset by reudcing of the size of the class which is in abundance while keeping all tuples in the minority classes. By doing such, the run-time are significantly improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsample4(data, nontox_frac, toxic_frac):\n",
    "    \n",
    "    data=data[['comment_text', 'non_toxic','mild_toxicity','toxic','severe_toxic','toxicity_level']]\n",
    "    non_toxic= data[data['toxicity_level']==1].sample(frac=nontox_frac,random_state=961)\n",
    "    toxic= data[data['toxicity_level']==3].sample(frac=toxic_frac,random_state=961)\n",
    "    dataB= non_toxic.append(toxic)\n",
    "    data= dataB.append(data[(data['toxicity_level']==2)|(data['toxicity_level']==4)])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fractions below correspond to each toxicity level.  They can be adjusted accordingly.\n",
    "train_df4 = get_subsample4(train_df, 0.01, .10)\n",
    "test_df4 = get_subsample4(test_df, 0.01, .10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4=train_df[['comment_text','non_toxic','mild_toxicity','toxic','severe_toxic','toxicity_level']]\n",
    "nonToxic= train_df4[train_df4['toxicity_level']==1].sample(frac=0.01,random_state=961)\n",
    "toxic = train_df4[train_df4['toxicity_level']==3].sample(frac=0.1,random_state=961)\n",
    "trainB= nonToxic.append(toxic)\n",
    "train_df4= trainB.append(train_df4[(train_df4['toxicity_level']==2)|(train_df4['toxicity_level']==4)])\n",
    "targetB= list(train_df4.columns.drop(['comment_text','toxicity_level']))\n",
    "labelB= train_df4[targetB].sum()\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = sns.barplot(x=labelB.index, y=labelB.values)\n",
    "ax.tick_params(labelsize=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Text Preprocessing \n",
    "\n",
    "The diversity and vastness of social media comments make it difficult to comprehend and capture the underlying trends and characteristics for comments with different toxicity levels. In details, keeping all words makes the dimensionality of each text extremely high, which makes classification more challenging. However, properly preprocessing the data, by reducing noise in the text, may improve classifier performance and speed up the classification process, thereby facilitating real-time sentiment analysis. Hence,in order to conduct data mining on online opinion data, the text preprocessing steps involve several stages, including removal of punctuation, lowering the text, removal of white spaces, stemming, removing stop words, handling negation, and finally tokenization and lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "def remove_punc(comment):\n",
    "    nonPunc=\"\".join([letter for letter in comment if letter not in string.punctuation])\n",
    "    return nonPunc\n",
    "\n",
    "#Lowering the text\n",
    "def toLower(comment):\n",
    "    return comment.lower()\n",
    "\n",
    "#Removing numbers \n",
    "def replace_numbers(comment):\n",
    "    \"\"\"Replace all interger occurrences in \n",
    "    list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', comment)\n",
    "\n",
    "#Removing whitespaces\n",
    "def remove_space(comment):\n",
    "    return comment.strip()\n",
    "\n",
    "#Tokenization\n",
    "def text2word(comment):\n",
    "    return word_tokenize(comment)\n",
    "\n",
    "#Removing Stop words\n",
    "def remove_stopW(words,stopWords):\n",
    "    return [word for word in words if word not in stopWords]\n",
    "\n",
    "#Lemmatization\n",
    "def lematizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_comm= [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemm_comm\n",
    "\n",
    "def lematizer_verb(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_comm= [lemmatizer.lemmatize(word,\"v\") for word in words]\n",
    "    return lemm_comm\n",
    "\n",
    "\n",
    "def clean_comment(comment):\n",
    "    comment= remove_punc(comment)\n",
    "    comment= toLower(comment)\n",
    "    comment= replace_numbers(comment)\n",
    "    comment= remove_space(comment)\n",
    "    words=text2word(comment)\n",
    "    words=remove_stopW(words,stopWords)\n",
    "    words=lematizer(words)\n",
    "    words=lematizer_verb(words)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4['comment_text'] = train_df4['comment_text'].apply(lambda x: clean_comment(x))\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_comment(x))\n",
    "train_df4= train_df4.reset_index(drop=True)\n",
    "train_df4.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Data Quality Assessment \n",
    "IN the final step for data preprocessing, we examined data quality of the 4 classes based on the Bag of Word model, where each row represents a specific text in corpus and each column represents a word in vocabulary. Based on the following plot on top terms per toxicity level, there is no significant difference between non_toxic and mild_toxicity. Hence, regarding the small distrbution in mild_toxicity class and the low level of uniqueness, we decided to drop mild_toxicity for model quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topNwords(data,topN=5):\n",
    "    vec= CountVectorizer(stop_words='english',analyzer='word').fit(data)\n",
    "    bagOfW= vec.transform(data)\n",
    "    sumWord= bagOfW.sum(axis=0)\n",
    "    wordFreq= [(word,sumWord[0,idx])for word, idx in vec.vocabulary_.items()]\n",
    "    wordFreq= sorted(wordFreq,key=lambda x:x[1], reverse=True)\n",
    "    topTerm= pd.DataFrame(wordFreq[:topN], columns=[\"Term\",\"Count\"])\n",
    "    return topTerm\n",
    "\n",
    "def topTermByClass(data, topN=10):\n",
    "    ''' Expecting the data with two columns, one with comment_text, the other\n",
    "    with its toxicity_level. Return a list of dataframes for each of the class in the toxicity_level,\n",
    "    where each dataframe contains topN terms and their tfidf value'''\n",
    "    dfs=[]\n",
    "    for level in np.unique(data['toxicity_level']):\n",
    "        idx= data['toxicity_level'].index[data['toxicity_level']==level]\n",
    "        topTerm= topNwords(data.loc[idx]['comment_text'],topN= 10)\n",
    "        dfs.append(topTerm)\n",
    "    #print(\"DFS:\"+str(dfs))\n",
    "    type=['non_toxic', 'mild_toxicity', 'toxic','severe_toxic']\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.suptitle(\"Top Terms Per Toxicity Level\",fontsize=10)\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    gridspec.GridSpec(2,2)\n",
    "    for termClass in range(2):\n",
    "        plt.subplot2grid((2,2),(0,termClass))\n",
    "        sns.barplot(x=dfs[termClass]['Term'],y=dfs[termClass]['Count'])\n",
    "        plt.title((\"Toxicity Class:{\"+type[termClass]+\"}\"),fontsize=8)\n",
    "        plt.xlabel('Word', fontsize=7)\n",
    "        plt.xticks(fontsize=7)\n",
    "        plt.yticks(fontsize=7)\n",
    "        plt.ylabel('Count', fontsize=7)\n",
    "    row2Ind= 2\n",
    "    #print(\"dfs[row2Ind]['Term']\"+str(dfs[row2Ind]['Term']))\n",
    "    #print(\"dfs[row2Ind]['Count']\"+str(dfs[row2Ind]['Count']))\n",
    "    for row2 in range(2):\n",
    "        plt.subplot2grid((2,2),(1,row2))\n",
    "        sns.barplot(x=dfs[row2Ind]['Term'],y=dfs[row2Ind]['Count'])\n",
    "        plt.title((\"Toxicity Class: {\"+type[row2Ind]+\"}\"),fontsize=8)\n",
    "        plt.xlabel('Word', fontsize=7)\n",
    "        plt.xticks(fontsize=7)\n",
    "        plt.yticks(fontsize=7)\n",
    "        plt.ylabel('Count', fontsize=7)\n",
    "        row2Ind +=1\n",
    "    plt.show()\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSub= train_df4[['comment_text','toxicity_level']]\n",
    "x= topTermByClass(trainSub,10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Final Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMild(data):\n",
    "    data = data[data.toxicity_level !=2]\n",
    "    data['toxicity_level'] = train_df['toxicity_level'].replace([3,4], [2,3])\n",
    "    data = data.drop(columns=['mild_toxicity'])       \n",
    "    data=data[['comment_text', 'non_toxic','toxic','severe_toxic','toxicity_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subsample(data, nontox_frac, toxic_frac, severetoxic_frac, chart_title):\n",
    "    \n",
    "    data=data[['comment_text', 'non_toxic','toxic','severe_toxic','toxicity_level']]\n",
    "\n",
    "    non_toxic= data[data['toxicity_level']==1].sample(frac=nontox_frac,random_state=961)\n",
    "    #print(\"Non toxic shape:\"+str(nonToxic.shape))\n",
    "\n",
    "    toxic= data[data['toxicity_level']==2].sample(frac=toxic_frac,random_state=961)\n",
    "    #print(\"Toxic shape:\"+str(nonToxic.shape))\n",
    "\n",
    "    severe_toxic = data[data['toxicity_level']==3].sample(frac=severetoxic_frac,random_state=961)\n",
    "    #print(\"Sever Toxic shape:\"+str(toxic.shape))\n",
    "\n",
    "    data = non_toxic.append(toxic).append(severe_toxic)\n",
    "    print(data[['toxicity_level']].value_counts())\n",
    "\n",
    "    #make sure the toxic comments does not include severe toxic comments\n",
    "    data.loc[data['severe_toxic'] == 1, 'toxic'] = 0\n",
    "\n",
    "    target= list(data.columns.drop(['comment_text','toxicity_level']))\n",
    "    label= data[target].sum()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    ax = sns.barplot(x=label.index, y=label.values)\n",
    "    ax.tick_params(labelsize=7)\n",
    "    plt.title(chart_title)\n",
    "    for i in ax.containers:\n",
    "        ax.bar_label(i,)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fractions below correspond to each toxicity level.  They can be adjusted accordingly.\n",
    "train_df = get_subsample(train_df, 0.01, .10, 1.0, \"Distribution of Training Data\")\n",
    "test_df = get_subsample(test_df, 0.01, .10, 1.0, \"Distribution of Testing Data\")\n",
    "\n",
    "print('Train df shape = ' +str(train_df.shape))\n",
    "print('Test df shape = ' +str(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments(train_df, test_df):\n",
    "    'clean comments in dataframes'\n",
    "\n",
    "    train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_comment(x))\n",
    "    test_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_comment(x))\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(df):\n",
    "    'reset index of df'\n",
    "\n",
    "    df= df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_dfs(file_name, dfs):\n",
    "    'store dataframes to file'\n",
    "\n",
    "    clean_data = dfs\n",
    "    with open(file_name, 'wb') as my_file_object:\n",
    "        pickle.dump(clean_data, my_file_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Final Set of Steps to Clean Data and Reduce Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fractions below correspond to each toxicity level.  They can be adjusted accordingly.\n",
    "train_df = get_subsample(train_df, 0.01, .10, 1.0, \"Distribution of Training Data\")\n",
    "test_df = get_subsample(test_df, 0.01, .10, 1.0, \"Distribution of Testing Data\")\n",
    "\n",
    "print('Train df shape = ' +str(train_df.shape))\n",
    "print('Test df shape = ' +str(test_df.shape))\n",
    "\n",
    "train_df, test_df = clean_comments(train_df, test_df)\n",
    "#test\n",
    "#train_df[\"comment_text\"].iloc[2]\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "   reset_index(df)\n",
    "\n",
    "#can input file name to correspond with percent of distributions\n",
    "store_dfs('clean_data1.p', [train_df, test_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"comment_text\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"comment_text\"].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lab.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
