{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validated Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_subset(pipe_params, subset):\n",
    "    'perform the pipeline(transform corpus to matrix and use to make model, use on test data) using the parameter dictionary on the data'\n",
    "\n",
    "    #set variables from input\n",
    "    pipe, parameters = pipe_params\n",
    "    train_data, train_labels, test_data, test_labels = subset\n",
    "    \n",
    "    def gridSearch(pipe, parameters):\n",
    "        'runs the grid search CV function with input pipe and parameters'\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator = pipe,\n",
    "            param_grid = parameters,\n",
    "            n_jobs = 2,  \n",
    "        )\n",
    "        return grid_search\n",
    "    \n",
    "    #set grid Search CV with input pipe and parameters\n",
    "    grid_search = gridSearch(pipe, parameters)\n",
    "    #fit the data to the model through the pipe and randomized CV\n",
    "    grid_search.fit(train_data,train_labels)\n",
    "\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    print('Pipeline = {}'.format(pipe))\n",
    "    \n",
    "    #formating print statments\n",
    "    print('Best parameters = ')\n",
    "    bestparams = []\n",
    "    if type(parameters) is list:\n",
    "        for param_dict in parameters:\n",
    "            if type(param_dict) is dict:\n",
    "                for param_name in sorted(param_dict.keys()):\n",
    "                    if param_name in best_parameters.keys():\n",
    "                        x = str(param_name) + ': ' + str(best_parameters[param_name])\n",
    "                        if x not in bestparams:\n",
    "                            bestparams.append(x)\n",
    "        print(*bestparams, sep = '\\n')\n",
    "                #print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "    else:\n",
    "        for param in sorted(parameters.keys()):\n",
    "            print(f\"{param}: {best_parameters[param]}\")\n",
    "                        \n",
    "\n",
    "    #test_accuracy = grid_search.score(test_data, test_labels)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    #print(\n",
    "       # \"Accuracy of the best parameters using the inner CV of \"\n",
    "      #  f\"the random search: {grid_search.best_score_:.3f}\"\n",
    "    #)\n",
    "    #print(f\"Accuracy on test set: {test_accuracy:.3f}\")\n",
    "    return best_estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['non-toxic', 'toxic', 'severe_toxic']\n",
    "#initialize dictionary for evaluation scores\n",
    "scores = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train, classifier):\n",
    "    \n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def y_preds(model, X_test):\n",
    "    'get the predicted y values'\n",
    "\n",
    "    y_preds = model.predict(X_test)\n",
    "    return y_preds\n",
    "\n",
    "def y_score1(model, X_test):\n",
    "    'get the predicted y values'\n",
    "\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    return y_score\n",
    "\n",
    "def y_score2(X_train, X_test, y_train, classifier):\n",
    "    'get the predicted y values'\n",
    "\n",
    "    #get scores for model functions which do not have the predict_proba method\n",
    "    return #y_score\n",
    "\n",
    "#https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "def conf_Ma(model, y_test, X_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2, in\n",
    "            zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(3,3)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "    #sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, cmap = 'Greens')\n",
    "    accuracy  = np.trace(cf_matrix) / float(np.sum(cf_matrix))\n",
    "    stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label' + stats_text)\n",
    "    return\n",
    "\n",
    "def ROC_AUC_analysis(y_train, y_test, y_score, target_names, classifier):\n",
    "    'Plot the ROC_AUC curve and return the average macro AUC value'\n",
    "\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    label_binarizer = LabelBinarizer().fit(y_train)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    # store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"weighted\"] = fpr_grid\n",
    "    tpr[\"weighted\"] = mean_tpr\n",
    "    roc_auc[\"weighted\"] = auc(fpr[\"weighted\"], tpr[\"weighted\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    plt.plot(\n",
    "        fpr[\"weighted\"],\n",
    "        tpr[\"weighted\"],\n",
    "        label=f\"weighted-average ROC curve (AUC = {roc_auc['weighted']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"ROC curve for {target_names[class_id]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"ROC curve for chance level (AUC = 0.5)\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(str(classifier) + \"\\nExtension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc['weighted']\n",
    "\n",
    "def analysis(subset, target_names, classifier):\n",
    "        \n",
    "        X_train, y_train, X_test, y_test = subset\n",
    "        model = fit_model(X_train, y_train, classifier)\n",
    "        y_score = y_score1(model, X_test)\n",
    "        conf_Ma(model, y_test, X_test)\n",
    "        ROC_AUC_analysis(y_train, y_test, y_score, target_names, classifier)\n",
    "        predict = y_preds(model, X_test)\n",
    "        print(classification_report(y_test, predict, target_names = target_names))\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "\n",
    "        scores['Classifier/Pipeline'].append(classifier)\n",
    "        scores['Accuracy'].append(accuracy)\n",
    "        scores['ROC_AUC'].append(roc_auc_score(y_test, y_score, average = 'weighted', multi_class ='ovo'))\n",
    "        for metric in [precision_score, recall_score, f1_score]:\n",
    "            score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
    "            scores[score_name].append(metric(y_test, predict, average = 'weighted'))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
