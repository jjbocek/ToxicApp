{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A Toxic Comment Identifier Application - Doc2Vec Data Transformation\n",
    "<a id='section_id2'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import savetxt\n",
    "import pickle\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Public Methods\n",
    "Below are public methods which can be called to generate vectors for a dataframe that is passed in."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A method to get a list of doc2vec vectors based on the model and dataframe passed in\n",
    "The dataframe should just consist of 1 column which contains the document or comment\n",
    "to be vectorized\n",
    ":param model : a doc2vec model that has already been initialized and defined\n",
    ":param data : the dataframe which contains the document or comment to be vectorized\n",
    ":return a list of vectors corresponding to the data passed in\n",
    "\"\"\"\n",
    "def get_doc2vec_vectors(model, data):\n",
    "    # #using default values for now\n",
    "    tokenized_comments = tokenize_comments(data)\n",
    "    tagged_documents = get_tagged_documents(tokenized_comments)\n",
    "\n",
    "    # build the vocabulary\n",
    "    # input a list of documents\n",
    "    model.build_vocab(x for x in tagged_documents)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(tagged_documents, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "    #print(\"Inferring \"+str(len(tokenized_comments)) +\" comments into doc2vec vectors.\")\n",
    "    vectors = infer_vectors(model, tokenized_comments, \"\")\n",
    "    return vectors\n",
    "\n",
    "\"\"\"\n",
    "A method that infers a list of vectors from a trained Doc2Vec model\n",
    ": param model : a Doc2Vec model which is already trained with vocab built\n",
    ": param input : a data frame to infer Doc2Vec vectors from\n",
    ": param save_file_name [OPTIONAL] : If a string is provided,\n",
    "the vectors will be saved using this file name.\n",
    "\"\"\"\n",
    "def infer_vectors(model, tokenized_comments, save_file_name):\n",
    "    #print(\"Inferring \"+str(len(tokenized_comments)) +\" comments into doc2vec vectors.\")\n",
    "    vectors = []\n",
    "    for comment in tokenized_comments:\n",
    "        #count = count + 1\n",
    "        #print(\"Vectorizing: \"+str(count)+\" comment.\")\n",
    "        vectors.append(model.infer_vector(comment))\n",
    "\n",
    "    #print(\"Created \"+str(len(vectors)) + \" doc2vec vectors.\")\n",
    "    #save to file if a file name is present\n",
    "    if save_file_name != \"\":\n",
    "        print(\"Saving vectors to file: \" + str(save_file_name))\n",
    "        savetxt(save_file_name, vectors)\n",
    "\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Methods\n",
    "\n",
    "Below are helper methods used by the public methods above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function to tokenize all data in a dataframe\n",
    ":param data: a dataframe containing comments to tokenize\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_comments(dataframe):\n",
    "    data = []\n",
    "    for row in dataframe:\n",
    "        data.append(tokenize_each_comment(row))\n",
    "    return data\n",
    "\"\"\"\n",
    "A function to tokenize a single comment\n",
    ":param data: a single comment to tokenize\n",
    "\"\"\"\n",
    "def tokenize_each_comment(comment):\n",
    "    temp = []\n",
    "    for j in word_tokenize(comment):\n",
    "        temp.append(j)\n",
    "    return temp\n",
    "\n",
    "\"\"\"\n",
    "A function to generate a list of tagged documents to train a\n",
    "Doc2Vec model\n",
    ":param list_of_tokenized_comments: A list of tokenized comments\n",
    "\"\"\"\n",
    "def tagged_document(list_of_tokenized_comments):\n",
    "  for x, ListOfWords in enumerate(list_of_tokenized_comments):\n",
    "    yield doc2vec.TaggedDocument(ListOfWords, [x])\n",
    "\n",
    "\"\"\"\n",
    "A function to get tagged documents from\n",
    "a list of tokenized comments\n",
    "\"\"\"\n",
    "def get_tagged_documents(list_of_tokenized_comments):\n",
    "    return list(tagged_document(list_of_tokenized_comments))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning of Doc2Vec Model\n",
    "\n",
    "To tune the parameters that we can customize for a Doc2Vec model, we used a Custom Rocchio and the NearestCentroid classifier as a model to validate the parameters.  After doing a lot of research, it was decided to create a custom method to tune the Doc2Vec parameters. T  First, we created a dictionary of the parameters I want to tune.\n",
    "\n",
    "The parameters we chose to tune was the following (Note, the descriptions below were taken from this source: https://medium.com/betacom/hyperparameters-tuning-tf-idf-and-doc2vec-models-73dd418b4d):\n",
    "* dm: it defines the training algorithm. If dm=1, PV-DM is used. Otherwise, PV-DBOW is employed.\n",
    "* vector_size: dimensionality of the feature vectors.\n",
    "* window: the maximum distance between the current and predicted word within a sentence.\n",
    "* hs: if 1, hierarchical softmax will be used for model training; if set to 0, and negative is non-zero, negative sampling will be used.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import itertools\n",
    "dm = [0, 1]\n",
    "vector_size = [500, 1000]\n",
    "window = [2,5]\n",
    "hs = [1]\n",
    "paramsList = [{'dm': item[0],\n",
    "               'vector_size': item[1],\n",
    "               'window': item[2],\n",
    "               'hs': item[3],\n",
    "               'negative': 0\n",
    "               } for item in\n",
    "                 list(itertools.product(*[dm,\n",
    "                                          vector_size,\n",
    "                                          window,\n",
    "                                          hs]))\n",
    "              ]\n",
    "\n",
    "#Note: commented out so it doesn't run when called from another notebook\n",
    "#print(\"The list of parameters for tuning the Doc2Vec Model:\"+str(paramsList))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Doc2Vec Model with Custom Rocchio Classifier\n",
    "Below we evaluated the Doc2Vec model using a custom Rocchio classifier which uses cosine distance as the metric.  For evaluation, we went with a simple metric which prints the accuracy of the classifier based on how it categorized the test data.\n",
    "\n",
    "The best accuracy we received was with the parameter settings below:\n",
    "\n",
    "Evaluating {'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}\n",
    "Rocchio classifier - Number of test instances classified correctly:1026 Percent Accuracy: 67.67810026385224"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "%run Toxic_App_Rocchio_Classifier.ipynb\n",
    "\n",
    "file_object = open('clean_data1.p', 'rb')\n",
    "clean_data = pickle.load(file_object)\n",
    "train_ds = clean_data[0]\n",
    "test_ds = clean_data[1]\n",
    "\n",
    "train_comments_df = train_ds['comment_text']\n",
    "test_comments_df = test_ds['comment_text']\n",
    "train_labels_df = train_ds['toxicity_level']\n",
    "test_labels_df = test_ds['toxicity_level']\n",
    "\n",
    "def evaluateDoc2VecParams():\n",
    "    # Tag docs\n",
    "    train_tokenized_comments = tokenize_comments(train_comments_df)\n",
    "    train_tagged_documents = get_tagged_documents(train_tokenized_comments)\n",
    "    scoreList = []\n",
    "    for param in paramsList:\n",
    "      print(\"Evaluating \"+str(param))\n",
    "      try:\n",
    "        d2v_model = doc2vec.Doc2Vec(train_tagged_documents,\n",
    "                        dm=param['dm'],\n",
    "                        vector_size=param['vector_size'],\n",
    "                        window=param['window'],\n",
    "                        min_count=1,\n",
    "                        epochs=10,\n",
    "                        hs=param['hs'],\n",
    "                        seed=516)\n",
    "        train_vectors = get_doc2vec_vectors(d2v_model,train_comments_df)\n",
    "\n",
    "        tokenized_test_comments = tokenize_comments(test_comments_df)\n",
    "        test_vectors = infer_vectors(d2v_model, tokenized_test_comments, \"\")\n",
    "\n",
    "        train_labels = np.array(train_labels_df)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        test_labels = np.array(test_labels_df)\n",
    "\n",
    "        # generate the prototype vectors\n",
    "        prototype_vectors = rocchio_train(train_vectors, train_labels)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        rocchio_evaluate(test_vectors, test_labels, prototype_vectors)\n",
    "      except Exception as error:\n",
    "        print(f'Cannot evaluate model with parameters {param} because of error: {error}')\n",
    "        continue\n",
    "    return scoreList\n",
    "\n",
    "#Note: commented out so it doesn't run when called from another notebook\n",
    "#evaluateDoc2VecParams()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Doc2Vec Model with Nearest Centroid Classifier\n",
    "Below we evaluated the Doc2Vec model using the Nearest Centroid classifier which uses euclidean distance as the metric.  Overall, for this model we achieve lower accuracy scores compared to the Custom Rocchio model above.\n",
    "\n",
    "The best accuracy we received was for the settings below:\n",
    "\n",
    "Evaluating {'dm': 0, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}\n",
    "Nearest Centroid classifier - Number of test instances classified correctly:960 Percent Accuracy: 63.3245382585752"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "%run Toxic_App_Rocchio_Classifier.ipynb\n",
    "\n",
    "file_object = open('clean_data1.p', 'rb')\n",
    "clean_data = pickle.load(file_object)\n",
    "train_ds = clean_data[0]\n",
    "test_ds = clean_data[1]\n",
    "\n",
    "train_comments_df = train_ds['comment_text']\n",
    "test_comments_df = test_ds['comment_text']\n",
    "train_labels_df = train_ds['toxicity_level']\n",
    "test_labels_df = test_ds['toxicity_level']\n",
    "\n",
    "def evaluateDoc2VecParams():\n",
    "    # Tag docs\n",
    "    train_tokenized_comments = tokenize_comments(train_comments_df)\n",
    "    train_tagged_documents = get_tagged_documents(train_tokenized_comments)\n",
    "    scoreList = []\n",
    "    for param in paramsList:\n",
    "      print(\"Evaluating \"+str(param))\n",
    "      try:\n",
    "        d2v_model = doc2vec.Doc2Vec(train_tagged_documents,\n",
    "                        dm=param['dm'],\n",
    "                        vector_size=param['vector_size'],\n",
    "                        window=param['window'],\n",
    "                        min_count=1,\n",
    "                        epochs=10,\n",
    "                        hs=param['hs'],\n",
    "                        seed=516)\n",
    "        train_vectors = get_doc2vec_vectors(d2v_model,train_comments_df)\n",
    "\n",
    "        tokenized_test_comments = tokenize_comments(test_comments_df)\n",
    "        test_vectors = infer_vectors(d2v_model, tokenized_test_comments, \"\")\n",
    "\n",
    "        train_labels = np.array(train_labels_df)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        test_labels = np.array(test_labels_df)\n",
    "\n",
    "        rocchio_classifier_nearest_centroid(train_vectors, train_labels, test_vectors, test_labels)\n",
    "\n",
    "      except Exception as error:\n",
    "        print(f'Cannot evaluate model with parameters {param} because of error: {error}')\n",
    "        continue\n",
    "    return scoreList\n",
    "\n",
    "#Note: commented out so it doesn't run when called from another notebook\n",
    "#evaluateDoc2VecParams()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "The 2 parameters below received the highest accuracies from both evaluations above:\n",
    "\n",
    "* Custom Rocchio: Evaluating {'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}\n",
    "* Nearest Centroid: Evaluating {'dm': 0, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}\n",
    "\n",
    "The only difference was with the window parameter. Considering this parameter controls the maximum distance between the current and predicted word of a sentence, we are inclined to go with the higher window.  Therefore, we will be using these parameters for the Doc2Vec model we use to transfor our data.\n",
    "\n",
    "Final set of parameters to use: 'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
