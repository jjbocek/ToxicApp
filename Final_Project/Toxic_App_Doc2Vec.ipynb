{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Doc2Vec - Methods to generate doc2vec vectors for Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Public Methods\n",
    "Below are public methods which can be called to generate vectors for a dataframe that is passed in."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import savetxt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A method to get a list of doc2vec vectors based on the model and dataframe passed in\n",
    "The dataframe should just consist of 1 column which contains the document or comment\n",
    "to be vectorized\n",
    "\n",
    ":param df : the dataframe which contains the document or comment to be vectorized\n",
    ":return a list of vectors corresponding to the data passed in\n",
    "\"\"\"\n",
    "def get_doc2vec_vectors(model, data):\n",
    "    # #using default values for now\n",
    "    tokenized_comments = tokenize_comments(data)\n",
    "    tagged_documents = get_tagged_documents(tokenized_comments)\n",
    "\n",
    "    # build the vocabulary\n",
    "    # input a list of documents\n",
    "    model.build_vocab(x for x in tagged_documents)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(tagged_documents, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "    #print(\"Inferring \"+str(len(tokenized_comments)) +\" comments into doc2vec vectors.\")\n",
    "    vectors = infer_vectors(model, tokenized_comments, \"\")\n",
    "    return vectors\n",
    "\n",
    "\"\"\"\n",
    "A method that infers a list of vectors from a trained Doc2Vec model\n",
    ": param model : a Doc2Vec model which is already trained with vocab built\n",
    ": param input : a data frame to infer Doc2Vec vectors from\n",
    ": param save_file_name [OPTIONAL] : If a string is provided,\n",
    "the vectors will be saved using this file name.\n",
    "\"\"\"\n",
    "def infer_vectors(model, tokenized_comments, save_file_name):\n",
    "    #print(\"Inferring \"+str(len(tokenized_comments)) +\" comments into doc2vec vectors.\")\n",
    "    vectors = []\n",
    "    for comment in tokenized_comments:\n",
    "        #count = count + 1\n",
    "        #print(\"Vectorizing: \"+str(count)+\" comment.\")\n",
    "        vectors.append(model.infer_vector(comment))\n",
    "\n",
    "    #print(\"Created \"+str(len(vectors)) + \" doc2vec vectors.\")\n",
    "    #save to file if a file name is present\n",
    "    if save_file_name != \"\":\n",
    "        print(\"Saving vectors to file: \" + str(save_file_name))\n",
    "        savetxt(save_file_name, vectors)\n",
    "\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function to tokenize all data in a dataframe\n",
    ":param data: a dataframe containing comments to tokenize\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_comments(dataframe):\n",
    "    data = []\n",
    "    for row in dataframe:\n",
    "        data.append(tokenize_each_comment(row))\n",
    "    return data\n",
    "\"\"\"\n",
    "A function to tokenize a single comment\n",
    ":param data: a single comment to tokenize\n",
    "\"\"\"\n",
    "def tokenize_each_comment(comment):\n",
    "    temp = []\n",
    "    for j in word_tokenize(comment):\n",
    "        temp.append(j)\n",
    "    return temp\n",
    "\n",
    "\"\"\"\n",
    "A function to generate a list of tagged documents to train a\n",
    "Doc2Vec model\n",
    ":param list_of_tokenized_comments: A list of tokenized comments\n",
    "\"\"\"\n",
    "def tagged_document(list_of_tokenized_comments):\n",
    "  for x, ListOfWords in enumerate(list_of_tokenized_comments):\n",
    "    yield doc2vec.TaggedDocument(ListOfWords, [x])\n",
    "\n",
    "\"\"\"\n",
    "A function to get tagged documents from\n",
    "a list of tokenized comments\n",
    "\"\"\"\n",
    "def get_tagged_documents(list_of_tokenized_comments):\n",
    "    return list(tagged_document(list_of_tokenized_comments))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
