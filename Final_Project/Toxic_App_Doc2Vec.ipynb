{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A Toxic Comment Identifier Application - Doc2Vec Data Transformation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Public Methods\n",
    "Below are public methods which can be called to generate vectors for a dataframe that is passed in."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import savetxt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A method to get a list of doc2vec vectors based on the model and dataframe passed in\n",
    "The dataframe should just consist of 1 column which contains the document or comment\n",
    "to be vectorized\n",
    ":param model : a doc2vec model that has already been initialized and defined\n",
    ":param data : the dataframe which contains the document or comment to be vectorized\n",
    ":return a list of vectors corresponding to the data passed in\n",
    "\"\"\"\n",
    "def get_doc2vec_vectors(model, data):\n",
    "    # #using default values for now\n",
    "    tokenized_comments = tokenize_comments(data)\n",
    "    tagged_documents = get_tagged_documents(tokenized_comments)\n",
    "\n",
    "    # build the vocabulary\n",
    "    # input a list of documents\n",
    "    model.build_vocab(x for x in tagged_documents)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(tagged_documents, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "    #print(\"Inferring \"+str(len(tokenized_comments)) +\" comments into doc2vec vectors.\")\n",
    "    vectors = infer_vectors(model, tokenized_comments, \"\")\n",
    "    return vectors\n",
    "\n",
    "\"\"\"\n",
    "A method that infers a list of vectors from a trained Doc2Vec model\n",
    ": param model : a Doc2Vec model which is already trained with vocab built\n",
    ": param input : a data frame to infer Doc2Vec vectors from\n",
    ": param save_file_name [OPTIONAL] : If a string is provided,\n",
    "the vectors will be saved using this file name.\n",
    "\"\"\"\n",
    "def infer_vectors(model, tokenized_comments, save_file_name):\n",
    "    #print(\"Inferring \"+str(len(tokenized_comments)) +\" comments into doc2vec vectors.\")\n",
    "    vectors = []\n",
    "    for comment in tokenized_comments:\n",
    "        #count = count + 1\n",
    "        #print(\"Vectorizing: \"+str(count)+\" comment.\")\n",
    "        vectors.append(model.infer_vector(comment))\n",
    "\n",
    "    #print(\"Created \"+str(len(vectors)) + \" doc2vec vectors.\")\n",
    "    #save to file if a file name is present\n",
    "    if save_file_name != \"\":\n",
    "        print(\"Saving vectors to file: \" + str(save_file_name))\n",
    "        savetxt(save_file_name, vectors)\n",
    "\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper Methods\n",
    "\n",
    "Below are helper methods used by the public methods above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function to tokenize all data in a dataframe\n",
    ":param data: a dataframe containing comments to tokenize\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_comments(dataframe):\n",
    "    data = []\n",
    "    for row in dataframe:\n",
    "        data.append(tokenize_each_comment(row))\n",
    "    return data\n",
    "\"\"\"\n",
    "A function to tokenize a single comment\n",
    ":param data: a single comment to tokenize\n",
    "\"\"\"\n",
    "def tokenize_each_comment(comment):\n",
    "    temp = []\n",
    "    for j in word_tokenize(comment):\n",
    "        temp.append(j)\n",
    "    return temp\n",
    "\n",
    "\"\"\"\n",
    "A function to generate a list of tagged documents to train a\n",
    "Doc2Vec model\n",
    ":param list_of_tokenized_comments: A list of tokenized comments\n",
    "\"\"\"\n",
    "def tagged_document(list_of_tokenized_comments):\n",
    "  for x, ListOfWords in enumerate(list_of_tokenized_comments):\n",
    "    yield doc2vec.TaggedDocument(ListOfWords, [x])\n",
    "\n",
    "\"\"\"\n",
    "A function to get tagged documents from\n",
    "a list of tokenized comments\n",
    "\"\"\"\n",
    "def get_tagged_documents(list_of_tokenized_comments):\n",
    "    return list(tagged_document(list_of_tokenized_comments))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning of Doc2Vec Model\n",
    "\n",
    "To tune the parameters that we can customize for a Doc2Vec model, I used the NearestCentroid classifier as a model to validate the parameters.  I decided to create a custom method to tune the Doc2Vec parameters.  First, I create a dictionary of the parameters I want to tune.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dm': 0, 'vector_size': 500, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 500, 'window': 5, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 0, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}, {'dm': 1, 'vector_size': 500, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 1, 'vector_size': 500, 'window': 5, 'hs': 1, 'negative': 0}, {'dm': 1, 'vector_size': 1000, 'window': 2, 'hs': 1, 'negative': 0}, {'dm': 1, 'vector_size': 1000, 'window': 5, 'hs': 1, 'negative': 0}]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "dm = [0,1]\n",
    "vector_size = [500, 1000]\n",
    "window = [2,5]\n",
    "hs = [1]\n",
    "paramsList = [{'dm': item[0],\n",
    "               'vector_size': item[1],\n",
    "               'window': item[2],\n",
    "               'hs': 1,\n",
    "               'negative': 0\n",
    "               } for item in\n",
    "                 list(itertools.product(*[dm,\n",
    "                                          vector_size,\n",
    "                                          window,\n",
    "                                          hs]))\n",
    "              ]\n",
    "\n",
    "print(paramsList)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#%run Toxic_App_Rochhio_Classifier.ipynb\n",
    "\n",
    "file_object = open('clean_data1.p', 'rb')\n",
    "clean_data = pickle.load(file_object)\n",
    "train_ds = clean_data[0]\n",
    "test_ds = clean_data[1]\n",
    "\n",
    "train_comments_df = train_ds['comment_text']\n",
    "test_comments_df = test_ds['comment_text']\n",
    "train_labels_df = train_ds['toxicity_level']\n",
    "test_labels_df = test_ds['toxicity_level']\n",
    "\n",
    "def evaluateDoc2VecParams():\n",
    "    # Tag docs\n",
    "    train_tokenized_comments = tokenize_comments(train_comments_df)\n",
    "    train_tagged_documents = get_tagged_documents(train_tokenized_comments)\n",
    "    scoreList = []\n",
    "    for param in paramsList:\n",
    "      print(\"Evaluating \"+str(param))\n",
    "      try:\n",
    "        d2v_model = doc2vec.Doc2Vec(train_tagged_documents,\n",
    "                        dm=param['dm'],\n",
    "                        vector_size=param['vector_size'],\n",
    "                        window=param['window'],\n",
    "                        min_count=1,\n",
    "                        epochs=10,\n",
    "                        hs=param['hs'],\n",
    "                        seed=516)\n",
    "        train_vectors = get_doc2vec_vectors(d2v_model,train_comments_df)\n",
    "\n",
    "        tokenized_test_comments = tokenize_comments(test_comments_df)\n",
    "        test_vectors = infer_vectors(d2v_model, tokenized_test_comments, \"\")\n",
    "\n",
    "        train_labels = np.array(train_labels_df)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        test_labels = np.array(test_labels_df)\n",
    "\n",
    "        rocchio_classifier_nearest_centroid(train_vectors, train_labels, test_vectors, test_labels)\n",
    "        # generate the prototype vectors\n",
    "        #prototype_vectors = rocchio_train(train_vectors, train_labels)\n",
    "\n",
    "        #classify test data using prototype vectors\n",
    "        #rocchio_evaluate(test_vectors, test_labels, prototype_vectors)\n",
    "      except Exception as error:\n",
    "        print(f'Cannot evaluate model with parameters {param} because of error: {error}')\n",
    "        continue\n",
    "    return scoreList\n",
    "\n",
    "#evaluateDoc2VecParams()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#Token\n",
    "min_DF = [1, 3, 10]\n",
    "max_DF = [0.6, 0.8, 0.9]\n",
    "#norm = ['l1','l2']\n",
    "\n",
    "#Dimensionality Reduction\n",
    "n_components = [115]\n",
    "\n",
    "#Classifer\n",
    "metric = ['euclidian', 'cosine']\n",
    "classes = ['1','2','3']\n",
    "\n",
    "rocchio_params =[\n",
    "    {\n",
    "        # 'token_value': [CountVectorizer(), TfidfVectorizer()],\n",
    "        # 'token_value__min_df': min_DF,\n",
    "        # 'token_value__max_df': max_DF,\n",
    "        'reduce_dim': ['passthrough'],\n",
    "        'clf__metric': metric\n",
    "    }\n",
    "    # {\n",
    "    #     'token_value': [CountVectorizer(), TfidfVectorizer()],\n",
    "    #     'token_value__min_df': min_DF,\n",
    "    #     'token_value__max_df': max_DF,\n",
    "    #     'reduce_dim': [TruncatedSVD()],\n",
    "    #     'reduce_dim__n_components': n_components,\n",
    "    #     'clf__metric': metric\n",
    "    # }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "pipe_rocchio = Pipeline(\n",
    "    [\n",
    "        ('token_value', 'passthrough'),\n",
    "        ('reduce_dim', 'passthrough'),\n",
    "        ('clf', NearestCentroid())\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def run_on_subset(pipe_params, subset):\n",
    "    'perform the pipeline(transform corpus to matrix and use to make model, use on test data) using the parameter dictionary on the data'\n",
    "\n",
    "    #set variables from input\n",
    "    pipe, parameters = pipe_params\n",
    "    train_data, train_labels, test_data, test_labels = subset\n",
    "\n",
    "    def gridSearch(pipe, parameters):\n",
    "        'runs the grid search CV function with input pipe and parameters'\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator = pipe,\n",
    "            param_grid = parameters,\n",
    "            n_jobs = 2,\n",
    "        )\n",
    "        return grid_search\n",
    "\n",
    "    #set grid Search CV with input pipe and parameters\n",
    "    grid_search = gridSearch(pipe, parameters)\n",
    "    #fit the data to the model through the pipe and randomized CV\n",
    "    grid_search.fit(train_data,train_labels)\n",
    "\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    print('Pipeline = {}'.format(pipe))\n",
    "\n",
    "    #formating print statments\n",
    "    print('Best parameters = ')\n",
    "    bestparams = []\n",
    "    if type(parameters) is list:\n",
    "        for param_dict in parameters:\n",
    "            if type(param_dict) is dict:\n",
    "                for param_name in sorted(param_dict.keys()):\n",
    "                    if param_name in best_parameters.keys():\n",
    "                        x = str(param_name) + ': ' + str(best_parameters[param_name])\n",
    "                        if x not in bestparams:\n",
    "                            bestparams.append(x)\n",
    "        print(*bestparams, sep = '\\n')\n",
    "                #print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "    else:\n",
    "        for param in sorted(parameters.keys()):\n",
    "            print(f\"{param}: {best_parameters[param]}\")\n",
    "\n",
    "\n",
    "    test_accuracy = grid_search.score(test_data, test_labels)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    print(\n",
    "        \"Accuracy of the best parameters using the inner CV of \"\n",
    "        f\"the random search: {grid_search.best_score_:.3f}\"\n",
    "    )\n",
    "    print(f\"Accuracy on test set: {test_accuracy:.3f}\")\n",
    "    return best_estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 10.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_nearest_centroid.py\", line 127, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse=[\"csr\", \"csc\"])\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "ValueError: could not convert string to float: 'filetonyukukmonumentinoldturkicalphabetjpg mine copyright violation filetonyukukmonumentinoldturkicalphabetjpg mine copyright violation file keep nominate deletion without evidence copyright violation'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_nearest_centroid.py\", line 127, in fit\n",
      "    X, y = self._validate_data(X, y, accept_sparse=[\"csr\", \"csc\"])\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 964, in check_X_y\n",
      "    X = check_array(\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 746, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\", line 872, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "ValueError: could not convert string to float: 'sorry break unsourced film plot much error unsourced interpretation lyric editor would quite within right remove require verifiable source'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/lisasaurus01/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'sorry break unsourced film plot much error unsourced interpretation lyric editor would quite within right remove require verifiable source'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_1252/4203198905.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m#test_labels_df = test_ds['toxicity_level']\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mrocchio_best\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_on_subset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpipe_rocchio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrocchio_params\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtrain_comments_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_comments_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_labels_df\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/c4/lz0fty3s6b3148w_cbvkndwm0000gn/T/ipykernel_1252/1730910767.py\u001B[0m in \u001B[0;36mrun_on_subset\u001B[0;34m(pipe_params, subset)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mgrid_search\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgridSearch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpipe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparameters\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0;31m#fit the data to the model through the pipe and randomized CV\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m     \u001B[0mgrid_search\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0mbest_parameters\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgrid_search\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    924\u001B[0m             \u001B[0mrefit_start_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    925\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 926\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    927\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    928\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    392\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_final_estimator\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"passthrough\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m                 \u001B[0mfit_params_last_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_params_steps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 394\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_final_estimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params_last_step\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    395\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    396\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_nearest_centroid.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    125\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"csc\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"csr\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"csc\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m         \u001B[0mis_X_sparse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0missparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_X_sparse\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshrink_threshold\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    579\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    580\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 581\u001B[0;31m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    582\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    583\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m    962\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"y cannot be None\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    963\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 964\u001B[0;31m     X = check_array(\n\u001B[0m\u001B[1;32m    965\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    966\u001B[0m         \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[1;32m    744\u001B[0m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcasting\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"unsafe\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 746\u001B[0;31m                     \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0morder\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0morder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    747\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    748\u001B[0m                 raise ValueError(\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m    870\u001B[0m               dtype='datetime64[ns]')\n\u001B[1;32m    871\u001B[0m         \"\"\"\n\u001B[0;32m--> 872\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    873\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    874\u001B[0m     \u001B[0;31m# ----------------------------------------------------------------------\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: 'sorry break unsourced film plot much error unsourced interpretation lyric editor would quite within right remove require verifiable source'"
     ]
    }
   ],
   "source": [
    "#[pipe_rocchio, rocchio_params]\n",
    "#[train_df_subset1, train_lab_subset1, test_df_subset1, test_lab_subset1]\n",
    "#train_comments_df = train_ds['comment_text']\n",
    "#test_comments_df = test_ds['comment_text']\n",
    "#train_labels_df = train_ds['toxicity_level']\n",
    "#test_labels_df = test_ds['toxicity_level']\n",
    "\n",
    "#rocchio_best = run_on_subset([pipe_rocchio, rocchio_params], [train_comments_df, train_labels_df, test_comments_df, test_labels_df])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
